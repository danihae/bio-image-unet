{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Siamese U-Net Quickstart"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The Siamese U-Net is an improvement on the original U-Net architecture. It adds an additional additional encoder that encodes an additional frame other than the frame that we are trying to predict. See [this paper](https://pubmed.ncbi.nlm.nih.gov/31927473/). This repository contains an implementation of this network.\n",
    "\n",
    "If you need help using a helper function, you can always try running `help(whichever_interesting_function)` or just look at the source code. If you need help using a class (one that is directly under the `siam_package` director), trying to understand the examples in this notebook probably will be more helpful than finding the documentation of that function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data preparation\n",
    "\n",
    "Because Siam UNet requires an additional input for training, we need to utilize an additional frame and use the appropriate dataloader for that. For the purpose of this notebook, I will call the frame which we are trying to infer \"current frame\", and the frame which is before the current frame the \"previous frame.\" "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### If you know which frame you are looking for"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataloader in `siam_unet_cosh` takes an image that results from concatenating the previous frame with the current frame. If you already know which frame of which movie you want to train on, you can create this concatenated data using `generate_siam_unet_input_imgs.py`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "movie_dir = '/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/test_data/new_microscope/21B11-shgGFP-kin-18-bro4.tif' # change this\n",
    "frame = 10 # change this\n",
    "out_dir = '/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/training_data/yokogawa/siam_data/image/' # change this\n",
    "\n",
    "\n",
    "\n",
    "from siam_package.helpers.generate_siam_unet_input_imgs import generate_coupled_image\n",
    "\n",
    "generate_coupled_image(movie_dir, frame, out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### If you don't know which frame you are looking for"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have frames and labels, but you don't know which frame of which movie each frame comes from, you can use  `find_frame_of_image`. This function takes your query and compares it against a list of tif files you specify through the parameter `search_space`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "image_name = f'/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/training_data/yokogawa/lateral_epidermis/image/83.tif'\n",
    "\n",
    "razer_local_search_dir = '/media/longyuxi/H is for HUGE/docmount backup/all_movies'\n",
    "tifs_names = ['21B11-shgGFP-kin-18-bro4', '21B25_shgGFP_kin_1_Pos0', '21C04_shgGFP_kin_2_Pos4', '21C26_shgGFP_Pos12', '21D16_shgGFPkin_Pos7']\n",
    "search_space = [razer_local_search_dir + '/' + t + '.tif' for t in tifs_names]\n",
    "\n",
    "from siam_package.helpers.find_frame_of_image import find_frame_of_image\n",
    "\n",
    "find_frame_of_image(image_name, search_space=search_space)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function not only outputs what it finds to stdout, but also creates a machine readable output, location of which specified by `machine_readable_output_filename`, about which frames it is highly confident with at locating (i.e. an MSE of < 1000 and matching frame numbers). This output can further be used by `generate_siam_unet_input_images.py`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from siam_package.helpers.generate_siam_unet_input_imgs import utilize_search_result\n",
    "\n",
    "utilize_search_result(f'/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/training_data/yokogawa/amnioserosa/search_result_mr.txt', f'/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/test_data/new_microscope', f'/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/training_data/yokogawa/amnioserosa/label/', f'/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/training_data/training_data/yokogawa/siam_amnioserosa_sanitize_test/')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, organize the labels and images in a way similar to this shown. An example can be found at `siam_package/training_data/yokogawa/siam_lateral_epidermis`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "siam_package/training_data/yokogawa/siam_lateral_epidermis\n",
    "|\n",
    "├── image\n",
    "│   ├── 105.tif\n",
    "│   ├── 111.tif\n",
    "│   ├── 120.tif\n",
    "│   ├── 121.tif\n",
    "│   ├── 1.tif\n",
    "│   ├── 2.tif\n",
    "│   ├── 3.tif\n",
    "│   ├── 5.tif\n",
    "│   ├── 7.tif\n",
    "│   └── 83.tif\n",
    "└── label\n",
    "    ├── 105.tif\n",
    "    ├── 111.tif\n",
    "    ├── 120.tif\n",
    "    ├── 121.tif\n",
    "    ├── 1.tif\n",
    "    ├── 2.tif\n",
    "    ├── 3.tif\n",
    "    ├── 5.tif\n",
    "    ├── 7.tif\n",
    "    └── 83.tif\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training is simple. For example:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from siam_package import *\n",
    "\n",
    "dataset = 'yokogawa/siam_lateral_epidermis'\n",
    "base_dir = '/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/'\n",
    "\n",
    "# path to training data (images and labels with identical names in separate folders)\n",
    "dir_images = f'{base_dir}/training_data/training_data/{dataset}/image/'\n",
    "dir_masks = f'{base_dir}/training_data/training_data/{dataset}/label/'\n",
    "\n",
    "print('starting to create training dataset')\n",
    "# create training data set\n",
    "data = DataProcess([dir_images, dir_masks], data_path='./data', dilate_mask=0, aug_factor=10, create=False, invert=False, clip_thres=(0.2, 99.8), dim_out=(256, 256), shiftscalerotate=(0, 0, 0))\n",
    "\n",
    "save_dir = f'{base_dir}/models/siam_bce_amnio'\n",
    "# create trainer\n",
    "training = Trainer(data ,num_epochs=500 ,batch_size=12, load_weights=False, lr=0.0001, n_filter=32, save_iter=True, save_dir=save_dir)\n",
    "\n",
    "training.start()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note here that the value of the `n_filter` parameter is set to `32`. The network won't break with a different value of this, but you need to use the same value for the Predict part."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predicting is simple as well. Just swap in the parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load package\n",
    "from siam_package import *\n",
    "import os\n",
    "os.nice(10)\n",
    "from  siam_package.helpers import tif_to_mp4\n",
    "\n",
    "base_dir = '/media/longyuxi/H is for HUGE/docmount backup/unet_pytorch/'\n",
    "out_dir = f'{base_dir}/predicted_out'\n",
    "model = f'{base_dir}/models/siam_bce_amnio/model_epoch_100.pth'\n",
    "\n",
    "tif_file = f'{base_dir}/training_data/test_data/new_microscope/21C04_shgGFP_kin_2_Pos4.tif'\n",
    "\n",
    "result_file = f'{out_dir}/siam_bce_amnio_100_epochs_21C04_shgGFP_kin_2_Pos4.tif'\n",
    "out_mp4_file = result_file[:-4] + '.mp4'\n",
    "\n",
    "print('starting to predict file')\n",
    "# predict file \n",
    "predict = Predict(tif_file, result_file, model, invert=False, resize_dim=(512, 512), n_filter=32)\n",
    "# convert to mp4\n",
    "tif_to_mp4.convert_to_mp4(result_file, output_file=out_mp4_file, normalize_to_0_255=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}